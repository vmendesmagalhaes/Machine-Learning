{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9XXBNYaKWHy2DvT5ey/7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmendesmagalhaes/machine-learning/blob/main/Predicao_umidade_madeira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variation of moisture content on wood prediction\n",
        "\n",
        "**Author:** Vitor Mendes Magalhaes\n",
        "\n",
        "**Email:** vitor.mendes.magalhaes@gmail.com\n",
        "\n",
        "**Last review:** Fev/2023"
      ],
      "metadata": {
        "id": "9-10UdlcZ2sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the libraries."
      ],
      "metadata": {
        "id": "9zeGdHc22pSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R0A8j7EeWB75"
      },
      "outputs": [],
      "source": [
        "import pandas as pandas\n",
        "import numpy as numpy\n",
        "from numpy import absolute\n",
        "from numpy import std\n",
        "from numpy import mean\n",
        "import matplotlib.pyplot as matplot\n",
        "import seaborn as seaborn\n",
        "from sklearn.model_selection import train_test_split as createDataPartition\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is importing the spreadsheet (.XLS file)."
      ],
      "metadata": {
        "id": "VkYihumGjWdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados = pandas.read_excel('datasetjan23.xls')"
      ],
      "metadata": {
        "id": "SbTwRtmBo9Qo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's create a new attribute, called *PERCENTUAL*. It means the percentage of weight loss in relation to the total amount of incoming wood logs in each pile."
      ],
      "metadata": {
        "id": "LyrokgrApkA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados['PERCENTUAL'] = (dados['ESTORNO'] * 100) / dados['TOT_ENTR']"
      ],
      "metadata": {
        "id": "bHyUJDmopxo-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the new attribute, we will remove the instances of stacks that did not have log outputs. In theory, if they do not represent an operational error, these piles continued to receive loads of wood, or were not yet closed, at the time of data capture.\n",
        "\n",
        "We will also remove instances of piles that have not had reversed charges, that is, that in theory there was no loss of moisture. In the case of wood logs stored outdoors (technically, it is called \"natural drying\"), under the conditions of the captured data, there will always be moisture loss."
      ],
      "metadata": {
        "id": "uNpS1ChEr1SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados.drop(dados[dados['TOT_SAID'] == 0].index, inplace=True)\n",
        "dados.drop(dados[dados['ESTORNO'] == 0].index, inplace=True)"
      ],
      "metadata": {
        "id": "uIiqzQgCr8ug"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After removing the instances, let's create copies of the original dataset, which will be used to train different problem solving models using supervised learning: classification models (for different classes) and regression models."
      ],
      "metadata": {
        "id": "EDiAvLNw39VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao = dados\n",
        "dfRegressao = dados"
      ],
      "metadata": {
        "id": "hIL7O7x74Sqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's start the pre-processing step, removing the attributes that are not interesting for creating the learning models to be applied.\n",
        "\n",
        "Once this is done, we will continue pre-processing the data, now with a selection of features, removing attributes and biasing the data to conform to the type of supervised learning they propose.\n",
        "\n",
        "To create the regression model, we will remove the stack identification (*PILHA*), the total outputs (*TOT_SAID*), the difference between the input and output (*DIFERENCA*), the dates (*DATA_INICIO* and *DATA_FIM*) and the amount reversed (*ESTORNO*).\n",
        "\n",
        "Thus, we make it clear that the class we want to predict is the *PERCENTUAL* of weight loss relative to the total wood input in the pile."
      ],
      "metadata": {
        "id": "9w4ZJM9v4Xgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfRegressao = dfRegressao.drop([\"PILHA\", \"TOT_SAID\", \"DIFERENCA\", \"DATA_INICIO\", \"DATA_FIM\", \"ESTORNO\"], axis=1)"
      ],
      "metadata": {
        "id": "6MUq60SD4rmu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the creation of classification models, let's create a new attribute, called *INTERVALOPERDA*. Loss ranges are nothing more than weight loss percentage ranges, eg between 0% and 5% loss; between 5% and 10% loss, and so on, as far as the data allows.\n",
        "\n",
        "To define the best weight loss percentage ranges, we will first analyze visually, using three different visual analysis tools: a *boxplot*, a summary of the *PERCENTUAL* attribute (which will dictate the ranges), and a histogram with the frequency of instances with different percentages."
      ],
      "metadata": {
        "id": "XN6RdFXO5bu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao.boxplot(column =['PERCENTUAL'], grid = True)"
      ],
      "metadata": {
        "id": "SvRSax9A52bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao['PERCENTUAL'].hist()"
      ],
      "metadata": {
        "id": "BQWEh4K87S_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao.describe()"
      ],
      "metadata": {
        "id": "r-PxLgr79Aug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a brief analysis, it is noticed that there are possible operational errors that can generate noise in the classification model, allowing them to be considered outliers.\n",
        "\n",
        "A clear example is the loss percentage. There are percentages that tend to 100%, which does not occur in this process; there are also percentages that tend to 0% - same situation.\n",
        "\n",
        "So let's remove the instances with losses above 40%."
      ],
      "metadata": {
        "id": "k3dSCowT9S74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao = dfClassificacao.drop(dfClassificacao[dados['PERCENTUAL'] >= 40].index)"
      ],
      "metadata": {
        "id": "eVxEJTPL9zoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, let's check again through the *boxplot*."
      ],
      "metadata": {
        "id": "CpxYef71-EaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao.boxplot(column =['PERCENTUAL'], grid = True)"
      ],
      "metadata": {
        "id": "ETCKYU_c-J9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can return to the creation of the attribute *INTERVALOPERDA*, establishing different intervals of loss percentages:\n",
        "\n",
        "CLASS | INTERVAL\n",
        "--- | ---\n",
        "1 | less than 10%\n",
        "2 | between 10% and 20%\n",
        "3 | between 20% and 30%\n",
        "4 | more than 30%"
      ],
      "metadata": {
        "id": "OMoSmpiQ-Xwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, linha in dfClassificacao.iterrows():\n",
        "  if (linha['PERCENTUAL'] <= 10):\n",
        "    dfClassificacao.loc[i, 'INTERVALOPERDA'] = 1;\n",
        "  elif ((linha['PERCENTUAL'] > 10) and (linha['PERCENTUAL'] <= 20)):\n",
        "    dfClassificacao.loc[i, 'INTERVALOPERDA'] = 2;\n",
        "  elif ((linha['PERCENTUAL'] > 20) and (linha['PERCENTUAL'] <= 30)):\n",
        "    dfClassificacao.loc[i, 'INTERVALOPERDA'] = 3;\n",
        "  else:\n",
        "    dfClassificacao.loc[i, 'INTERVALOPERDA'] = 4;"
      ],
      "metadata": {
        "id": "1_scqdvP_rZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the *INTERVALOPERDA* attribute is created, let's remove the attributes that will not be important for the creation of the learning models - more specifically, the classification.\n",
        "\n",
        "In this sense, we will remove the stack identification (*PILHA*), the dates (*DATA_INICIO* and *DATA_FIM*, the output weight (*TOT_SAID*), the difference between the input and output totals (*DIFERENCA*) , the amount reversed (*ESTORNO*) and the percentage (*PERCENTUAL*).\n",
        "\n",
        "Thus, we make it clear that the class we want to predict is the *INTERVALOPERDA*, just created, which contains the interval (in percentage) of wood weight loss."
      ],
      "metadata": {
        "id": "kJo0hEjXUU3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfClassificacao = dfClassificacao.drop([\"PILHA\", \"DATA_INICIO\", \"DATA_FIM\", \"TOT_SAID\", \"DIFERENCA\", \"ESTORNO\", \"PERCENTUAL\"], axis=1)"
      ],
      "metadata": {
        "id": "ln9OmaUNUyfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the attribute (class) that will be our target, and removing the attributes that will not be predictors of our models, we will check the balance of the dataset."
      ],
      "metadata": {
        "id": "nDWfMrp2pE2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dfClassificacao['INTERVALOPERDA'].value_counts())"
      ],
      "metadata": {
        "id": "zHOCrgJPpllu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly observe that the largest number of instances is represented in classes 1 and 2, that is, the vast majority of the piles present in the dataset have a weight loss (due to the loss of moisture content) of less than 20%.\n",
        "\n",
        "We will verify the integrity of these data, analyzing the moisture losses from the piles in relation to the number of days that the logs remained stored (in the same piles)."
      ],
      "metadata": {
        "id": "BMzbjanopy55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dfClassificacao['QTD_DIAS'].describe())"
      ],
      "metadata": {
        "id": "e7X6S35iqHZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering, then, that when it comes to loss of moisture content in wood stored outdoors (*natural drying*) 20% is a small percentage, it can be easily inferred that these logs, due to the large number of days average battery life (81 days), had already been cut longer."
      ],
      "metadata": {
        "id": "0qZGUL4sqaNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As there are non-numeric predictor attributes, we need to treat them, transforming them into numerical attributes so that we can proceed with the training of the learning models (in Weka, in R and in other environments/languages, this transformation is not necessary, but it is need to analyze possible differences in the prediction result).\n",
        "\n",
        "Let's do the *one-hot-encoding* for categorical attributes that don't have any need for sorting, like product, bark, and species."
      ],
      "metadata": {
        "id": "iQrV5sGVIY5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "atributos_a_codificar = ['PRODUTO', 'CASCA', 'ESPECIE', 'DIAMETRO']\n",
        "\n",
        "atributos_codificados = pandas.get_dummies(dfClassificacao[atributos_a_codificar])\n",
        "\n",
        "dfClassificacao = dfClassificacao.drop(atributos_a_codificar, axis=1).join(atributos_codificados)"
      ],
      "metadata": {
        "id": "ybIpFNznJGrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has not yet been partitioned, and the tendency is for the partitions to reproduce the general behavior of the dataset. So let's split it before solving the class imbalance problem."
      ],
      "metadata": {
        "id": "IZXhmAHDWgjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados_treino, dados_teste, classe_treino, classe_teste = createDataPartition(dfClassificacao, dfClassificacao['INTERVALOPERDA'], test_size=0.25)\n"
      ],
      "metadata": {
        "id": "_KD4JvGPqk-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classe_teste.head(10)"
      ],
      "metadata": {
        "id": "TkqTPrH5sTgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, with the dataset partitioned with 75% of the data separated for training, and 25% for testing, we can proceed with the training of the models, with the data already biased towards supervised learning - classification.\n",
        "\n",
        "Let's then instantiate the classification model using the *Random Forest* (RF) algorithm."
      ],
      "metadata": {
        "id": "QLHaGszRto8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "8WDlsvYauHDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is instantiated, it is time to train the classification model."
      ],
      "metadata": {
        "id": "DMALP5sEp3L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(dados_treino, classe_treino)"
      ],
      "metadata": {
        "id": "_bwCUeJ-uTOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained (using both the training data and the training class), the next step is to verify the predictions, that is, what in fact the created model was able to learn from the available data."
      ],
      "metadata": {
        "id": "0G50JebvqEJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicoes = rf.predict(dados_teste)"
      ],
      "metadata": {
        "id": "I2ww95NPjJa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping the predictions in a variable, we will check through a confusion matrix the results of the learning model predictions using classification with RF."
      ],
      "metadata": {
        "id": "Txiint5_qXso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(classe_teste, predicoes)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "id": "JSuYLsagqiKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the confusion matrix above, one can notice the assertiveness of the chosen method. But we need to make the information clearer.\n",
        "\n",
        "For this, we are going to use some metrics, starting with the accuracy of the model, that is, how much, in percentage, the model is able to hit its predictions (always based on the data that the model had at its disposal)."
      ],
      "metadata": {
        "id": "wp0Br_bxqphs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Acurácia:\", metrics.accuracy_score(classe_teste, predicoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXnGx-baqmpG",
        "outputId": "57357762-1162-4404-d8e7-ab54e8d50801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9180327868852459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to keep in mind that, although accuracy is the proportion of correct predictions in relation to the total number of predictions, it is a simple metric, that is, it may not always be the most suitable metric - especially in unbalanced datasets."
      ],
      "metadata": {
        "id": "r9vOjejZsfXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcule a importância das features\n",
        "importance = rf.feature_importances_\n",
        "\n",
        "importance\n",
        "\n",
        "# Crie um gráfico para visualizar a importância das features\n",
        "matplot.barh(dados_teste.columns, importance)\n",
        "#matplot.xlabel('Importância da Feature')\n",
        "#matplot.ylabel('Feature')\n",
        "#matplot.title('Importância das Features no Modelo Random Forest')\n",
        "\n",
        "# Mostre o gráfico\n",
        "#matplot.show()"
      ],
      "metadata": {
        "id": "LphM_zYtvFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dfRegressao.head(10)\n",
        "\n",
        "dados_treino, dados_teste, classe_treino, classe_teste = createDataPartition(dfRegressao, dfClassificacao['ESTORNO'], test_size=0.25)"
      ],
      "metadata": {
        "id": "7FZ0jkLxVP-8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = Lasso(alpha=1.0)"
      ],
      "metadata": {
        "id": "S4THUXptVy8Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "h74_mDTtWWa9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(lasso, dados_treino, classe_treino, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)"
      ],
      "metadata": {
        "id": "otXwj4N0W_jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = absolute(scores)"
      ],
      "metadata": {
        "id": "IAwW0nIUXMsX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfvJF-AwXZqM",
        "outputId": "9ef32c15-6420-4d24-d2e8-ca229d44193c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MAE: nan (nan)\n"
          ]
        }
      ]
    }
  ]
}